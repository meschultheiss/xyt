{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance: GPSAnalytics()\n",
    "In this notebook you can find all the necessary steps to compute descriptive statistics on the staypoint dataframe. It also requires the leg for computing distances for instance.\n",
    "\n",
    "The objective for the library user is to get two dafaframes :\n",
    "- the df at the end of PART 1 through the following pipeline\n",
    "    - `check_inputs(leg, staypoint)` To be done: A small function to check if the input data have the right columns else ask user to adapt input data\n",
    "    - `split_overnight()`\n",
    "    - `spatial_clustering()`\n",
    "    - `get_metrics()`\n",
    "    \n",
    "- the df at the end of PART 2\n",
    "    - `get_daily_metrics()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopandas import sjoin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import DistanceMetric\n",
    "from sklearn.cluster import DBSCAN\n",
    "import datetime\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "from functions_preprocessing import *\n",
    "#from functions_prep import *\n",
    "\n",
    "pd.set_option('display.max_columns', 999)\n",
    "\n",
    "#nrows = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load staypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# READ FILES\n",
    "act = pd.read_pickle('sample_data/staypoint_sample_panel.pkl').reset_index()\n",
    "act.rename(columns={'IDNO':'user_id', 'id':'activity_id'}, inplace=True)\n",
    "del act['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Extract longitude and latitude into separate columns\n",
    "act['lon'] = act['geometry'].apply(lambda point: point.x)\n",
    "act['lat'] = act['geometry'].apply(lambda point: point.y)\n",
    "#Parse the activity df to datetime and geopandas\n",
    "act = parse_time_geo_data(act, geo_columns=['lon','lat'], datetime_format='%Y-%m-%d %H:%M:%S', CRS2='EPSG:2056')\n",
    "del act['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load legs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "leg = pd.read_pickle('sample_data/leg_sample_panel.pkl').reset_index()\n",
    "leg.rename(columns={'id':'leg_id', 'IDNO':'user_id'}, inplace=True)\n",
    "leg['started_at'] = pd.to_datetime(leg['started_at'])\n",
    "leg['finished_at'] = pd.to_datetime(leg['finished_at'])\n",
    "\n",
    "# Add the leg destination activity_id\n",
    "leg = find_next_activity_id(leg, act)\n",
    "\n",
    "# Add a 'length' column in meters\n",
    "leg = gpd.GeoDataFrame(leg, geometry='geometry', crs='EPSG:4327')\n",
    "leg['length'] = leg.to_crs(crs='EPSG:2056').length\n",
    "\n",
    "# Calculate the duration in seconds and add a 'duration' column in minutes\n",
    "leg['duration'] = (leg['finished_at'] - leg['started_at']).dt.total_seconds() / 60\n",
    "\n",
    "leg.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data format**\n",
    "\n",
    "In order to perform Part 1, you must have a staypoint df and a leg df with at least the following columns : \n",
    "```python\n",
    "staypoint.columns = ['activity_id', 'started_at', 'finished_at',\n",
    "       'purpose', 'user_id', 'lon', 'lat']\n",
    "```\n",
    "```python\n",
    "leg.columns = ['leg_id', 'started_at', 'finished_at',\n",
    "       'detected_mode', 'mode', 'user_id', 'geometry', 'next_activity_id',\n",
    "       'length', 'duration']\n",
    "```\n",
    "Pay attention to the format of (in particular) the columns with datetimes or geometries.\n",
    "Also, having a `purpose == 'home'`will help complete the calculations.\n",
    "\n",
    "**XYT instance implementation**\n",
    "\n",
    "Output of part 1 is an extended staypoint df with extra columns\n",
    "```python\n",
    "extended_staypoint = GPSAnalytics().metrics()\n",
    "extended_staypoint.columns = ['leg_id', 'started_at', 'finished_at',\n",
    "       'detected_mode', 'mode', 'user_id', 'geometry', 'next_activity_id',\n",
    "       'length', 'duration''cluster', 'cluster_size', 'cluster_info', 'location_id',\n",
    "       'peak', 'first_dep', 'last_arr', 'home_loop', 'daily_trip_dist',\n",
    "       'num_trip', 'max_dist', 'min_dist', 'max_dist_from_home',\n",
    "       'dist_from_home', 'home_location_id', 'weekday']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `GPSAnalytics().metrics.split_overnight()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#split the overnight activity into last and first activities\n",
    "act = split_overnight(act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `GPSAnalytics().metrics.spatial_clustering()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Aggregate the locations of most visited places per user_id and per imputed_purpose\n",
    "act = spatial_clustering(act, purpose_col='purpose')\n",
    "\n",
    "#Label the Most Visited Places\n",
    "act = cluster_info(act,purpose_col='purpose')\n",
    "\n",
    "#Aggregate the 100m-neighbooring lon,lat pairs\n",
    "db = DBSCAN(eps=100/3671000, min_samples=2, metric='haversine', algorithm='ball_tree')\n",
    "cl = db.fit_predict(np.deg2rad(act[['lon','lat']]))\n",
    "for cluster in np.unique(cl):\n",
    "    if cluster != -1:\n",
    "        act.loc[cl == cluster, 'lon'] = act.loc[cl == cluster, 'lon'].mean()\n",
    "        act.loc[cl == cluster, 'lat'] = act.loc[cl == cluster, 'lat'].mean()\n",
    "#Add location id for unique lon,lat pairs\n",
    "act['location_id'] = np.nan\n",
    "for counter, lon, lat in act.groupby(['lon','lat'], as_index=False).size()[['lon','lat']].itertuples(index=True):\n",
    "    act.loc[(act.lon == lon) & (act.lat == lat), 'location_id'] = counter\n",
    "act.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `GPSAnalytics().metrics.get_metrics()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derive additional variables\n",
    "df = act.copy()\n",
    "#DAILY USER_ID: Add user_ids per day\n",
    "df.insert(1, 'user_id_day', df['user_id'] + '_' + df.started_at.dt.year.astype(str) + df.started_at.dt.month.astype(str).str.zfill(2) + df.started_at.dt.day.astype(str).str.zfill(2))\n",
    "#PEAK HOURS: Add boolean if trip starts (i.e. activity ends) in peak hour\n",
    "morning = [datetime.datetime(2021,1,1,6,30).time(),datetime.datetime(2021,1,1,9,0).time()]\n",
    "noon = [datetime.datetime(2021,1,1,12,0).time(),datetime.datetime(2021,1,1,14,0).time()]\n",
    "evening = [datetime.datetime(2021,1,1,16,30).time(),datetime.datetime(2021,1,1,19,0).time()]\n",
    "df['peak'] = 0\n",
    "df.loc[(df.finished_at.dt.time > morning[0]) & (df.finished_at.dt.time < morning[1]), 'peak'] = 'morning_peak'\n",
    "df.loc[(df.finished_at.dt.time > noon[0]) & (df.finished_at.dt.time < noon[1]), 'peak'] = 'noon_peak'\n",
    "df.loc[(df.finished_at.dt.time > evening[0]) & (df.finished_at.dt.time < evening[1]), 'peak'] = 'evening_peak'\n",
    "#Get the time of first departure / last arrival\n",
    "df['first_dep'] = np.nan\n",
    "df['last_arr'] = np.nan\n",
    "df['home_loop'] = 0\n",
    "df['daily_trip_dist'] = np.nan\n",
    "df['num_trip'] = np.nan\n",
    "df['max_dist'] = np.nan\n",
    "df['min_dist'] = np.nan\n",
    "df['max_dist_from_home'] = np.nan\n",
    "df['dist_from_home'] = np.nan\n",
    "df['home_location_id'] = np.nan\n",
    "\n",
    "location = act[['lon','lat','location_id']].copy().sort_values('location_id').reset_index(drop=True)\n",
    "location.drop_duplicates(ignore_index=True,inplace=True)\n",
    "od_matrix_kms = pd.DataFrame(DistanceMetric.get_metric('haversine').pairwise(location[['lat','lon']].to_numpy())*6373, columns=location.location_id.unique(), index=location.location_id.unique())\n",
    "\n",
    "\n",
    "#Create functions to be parralelized\n",
    "def func1(df):\n",
    "    import datetime\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    ## Clean up the first / last activity of the day\n",
    "    #CASE 1. Manage cases where the first/last act is at home but started sometime in the morning/afternoon --> set started_at == 00:00:01 / finished_at == 23:59:59\n",
    "    #Set a df with only the first/last activities of user-days (NB DO NOT RESET OR IGNORE INDEXES): \n",
    "    first_act = df.loc[(df.drop_duplicates(subset=['user_id_day'], keep='first').index)].copy()\n",
    "    last_act = df.loc[(df.drop_duplicates(subset=['user_id_day'], keep='last').index)].copy()\n",
    "    \n",
    "    case1f = first_act[(first_act.started_at.dt.time > datetime.time(0, 0, 1)) & (first_act.started_at.dt.time < datetime.time(12, 0, 0)) & (first_act.imputed_purpose == 'Home')]\n",
    "    case1l = last_act[(last_act.finished_at.dt.time > datetime.time(12, 0, 0)) & (last_act.finished_at.dt.time < datetime.time(23, 59, 59)) & (last_act.imputed_purpose == 'Home')]\n",
    "    df.loc[case1f.index, 'started_at'] = pd.to_datetime(df.loc[case1f.index, 'started_at'].dt.date.astype(str)+\"T00:00:01Z\", format=\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    df.loc[case1l.index, 'finished_at'] = pd.to_datetime(df.loc[case1l.index, 'finished_at'].dt.date.astype(str)+\"T23:59:59Z\", format=\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    #Recalculate the durations\n",
    "    df.loc[case1l.index.append(case1f.index), 'duration'] = (df.loc[case1l.index.append(case1f.index), 'finished_at'] - df.loc[case1l.index.append(case1f.index), 'started_at']) / np.timedelta64(1, 's')\n",
    "    \n",
    "    #CASE 2. Drop all the user_id_day for wich the first / last activity does not starts / ends at 00:00:01 / 23:59:59\n",
    "    len_before = len(df)\n",
    "    #Set a df with only the first/last activities of user-days (NB DO NOT RESET OR IGNORE INDEXES): \n",
    "    first_act = df.loc[(df.drop_duplicates(subset=['user_id_day'], keep='first').index)].copy()\n",
    "    last_act = df.loc[(df.drop_duplicates(subset=['user_id_day'], keep='last').index)].copy()\n",
    "            \n",
    "    #Clean the user_id_day with home only\n",
    "    index_to_drop = []\n",
    "    for user_id in df.user_id_day.unique():\n",
    "        try:\n",
    "            if (len(df.loc[df.user_id_day == user_id, 'location_id'].unique()) == 1) & ('Home' in df.loc[df.user_id_day == user_id, 'imputed_purpose'].unique()):\n",
    "                index_to_drop.extend(df.loc[(df.user_id_day == user_id)].index[1:].tolist())\n",
    "                df.loc[(df.user_id_day == user_id), 'duration'] == df.loc[(df.user_id_day == user_id), 'duration'].sum()\n",
    "                df.loc[(df.user_id_day == user_id), 'started_at'] == df.loc[(df.user_id_day == user_id), 'started_at'].min()\n",
    "                df.loc[(df.user_id_day == user_id), 'finished_at'] == df.loc[(df.user_id_day == user_id), 'finished_at'].max()\n",
    "        except (ValueError):\n",
    "            continue\n",
    "    df = df.loc[~df.index.isin(index_to_drop)].copy()\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    df = df.loc[(~df.user_id_day.isin(first_act.loc[first_act.started_at.dt.time != datetime.time(0, 0, 1), 'user_id_day'].array))]\n",
    "    df = df.loc[(~df.user_id_day.isin(last_act.loc[last_act.finished_at.dt.time != datetime.time(23, 59, 59), 'user_id_day'].array))]\n",
    "    #len_after = len(df)\n",
    "    #print('Warning: clean up operation reduced the df lenght by -' + str(\"{:.1f}\".format((len_before-len_after)*100/len_before)) + ' %')\n",
    "    ##Most user-days start at home:\n",
    "    #print('But note that there are still some weird cases like a day starting with Shopping. Those cases are however very few :' + \"\\n\" + df.drop_duplicates(subset=['user_id_day'], keep='first').groupby(by='imputed_purpose').count()['user_id_day'].to_string())\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "##!!!ATTENTION: WE NEED THE LEG DATA HERE, NOT A GOOD PRACTICE TO ADD IT IN A FUNCTION LIKE THAT - BUT DUE TO MULTIPROCESSING\n",
    "##!!! ALSO THERE ARE LOTS OF COLUMN NAME DEPENDENCIES HERE... \n",
    "def func2(df, leg):\n",
    "    import datetime\n",
    "    import geopandas\n",
    "    #Compute miscellaneous additional variables\n",
    "    for user_id in df.user_id_day.unique():\n",
    "        try:\n",
    "            if len(df.loc[(df.user_id_day == user_id)]) > 1:\n",
    "                df.loc[(df.user_id_day == user_id), 'first_dep'] = df.loc[(df.user_id_day == user_id), 'finished_at'].dt.time.min()\n",
    "                df.loc[(df.user_id_day == user_id), 'last_arr'] = df.loc[(df.user_id_day == user_id), 'started_at'].dt.time.max()\n",
    "            if sum(df.loc[(df.user_id_day == user_id), 'imputed_purpose'].isin(['Home', 'home'])) > 1:\n",
    "                df.loc[(df.user_id_day == user_id), 'home_loop'] = sum(df.loc[(df.user_id_day == user_id), 'imputed_purpose'].isin(['Home', 'home'])) - 1\n",
    "            #find from legs the actual trip distance\n",
    "            date = datetime.datetime.strptime(user_id[-8:], \"%Y%m%d\").date() #retrieve the date of the concerned activities from the user_id_day string\n",
    "            condition1 = leg.next_activity_id.isin(df.loc[(df.user_id_day == user_id), 'activity_id'].tolist()) #spot all the legs tracked to reach the activities\n",
    "            condition2 = leg.started_at.dt.date == date #match the dates\n",
    "            df.loc[(df.user_id_day == user_id), 'daily_trip_dist'] = leg.loc[(condition1) & (condition2), 'length'].sum()    \n",
    "            #return the number of trips between activities\n",
    "            df.loc[(df.user_id_day == user_id), 'num_trip'] = len(df[df.user_id_day == user_id]) - 1\n",
    "        except (ValueError):\n",
    "            pass\n",
    "            \n",
    "    return df\n",
    "\n",
    "def func3(df, od_matrix_kms):\n",
    "    from sklearn.metrics import DistanceMetric\n",
    "    import numpy as np\n",
    "    import math\n",
    "    \n",
    "    #Compute the distances between locations\n",
    "    for user_id in df.user_id_day.unique():\n",
    "        try:\n",
    "            #Compute max/min distance between all activity locations\n",
    "            od_pairs = df.loc[df.user_id_day == user_id, ['lon', 'lat']].drop_duplicates(ignore_index=True)\n",
    "            od_dist = DistanceMetric.get_metric('haversine').pairwise(od_pairs[['lat','lon']].to_numpy())*6373\n",
    "            if len(od_dist) > 1:\n",
    "                df.loc[df.user_id_day == user_id, 'max_dist'] = od_dist.max().astype(int) * 1000\n",
    "                df.loc[df.user_id_day == user_id, 'min_dist'] = od_dist[np.nonzero(od_dist)].min().astype(int) * 1000 #get the min among non-null values\n",
    "            else:\n",
    "                df.loc[df.user_id_day == user_id, 'max_dist'] = 0\n",
    "                df.loc[df.user_id_day == user_id, 'min_dist'] = 0\n",
    "            #Compute max distance from home\n",
    "            home_locations = df.loc[(df.user_id_day == user_id) & (df.imputed_purpose.str.lower() == \"home\"), ['location_id', 'cluster_size']]\n",
    "            if len(home_locations.location_id.unique()) > 1:\n",
    "                home_id = home_locations.loc[home_locations.cluster_size.idxmax(), 'location_id']\n",
    "            else:\n",
    "                home_id = home_locations.location_id.mean()\n",
    "            if math.isnan(home_id) == False:\n",
    "                all_id = df.loc[(df.user_id_day == user_id) & (df.location_id != home_id), 'location_id']\n",
    "                df.loc[df.user_id_day == user_id, 'max_dist_from_home'] = od_matrix_kms.loc[od_matrix_kms.index.isin(all_id), home_id].max() * 1000\n",
    "                df.loc[df.user_id_day == user_id, 'home_location_id'] = home_id\n",
    "        except (ValueError):\n",
    "            continue\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import datetime\n",
    "from functools import partial\n",
    "#RUN PARRALEL FUNCTIONS FUNC0. FUNC1 & FUNC2\n",
    "#BE CAREFUL THIS PART CAN BE LONG\n",
    "#MAKE A PROGRESS BAR ?\n",
    "\n",
    "df.rename(columns={'purpose':'imputed_purpose'}, inplace=True)\n",
    "\n",
    "for func in [func1, func2, func3]:\n",
    "    cores = mp.cpu_count()\n",
    "    #split the df in as many array as the machine has cores\n",
    "    user_ids = np.array_split(df.user_id_day.unique(), cores, axis=0)\n",
    "    df_split = []\n",
    "    for u in user_ids:\n",
    "        df_split.append(df.loc[df.user_id_day.isin(u.tolist())])\n",
    "    # create the multiprocessing pool\n",
    "    pool = Pool(cores)\n",
    "    # process the DataFrame by mapping function to each df across the pool\n",
    "    if func == func2:\n",
    "        func2_partial = partial(func2, leg=leg)\n",
    "        df_out = np.vstack(pool.map(func2_partial, df_split))\n",
    "    elif func == func3:\n",
    "        func3_partial = partial(func3, od_matrix_kms=od_matrix_kms)\n",
    "        df_out = np.vstack(pool.map(func3_partial, df_split))\n",
    "    else:\n",
    "        df_out = np.vstack(pool.map(func, df_split))\n",
    "    \n",
    "    # return the df\n",
    "    df = pd.DataFrame(df_out, columns=df.columns)\n",
    "    \n",
    "    # close down the pool and join\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    pool.clear()\n",
    "    \n",
    "    if func == func2:\n",
    "        #drop the days with only one obesrvation and small connection duration\n",
    "        df.drop(df[(df.first_dep.isna()) & (df.duration < 43200)].index, inplace=True) #43200sec is 12 hours\n",
    "        #Add weekdays\n",
    "        df['weekday'] = df.started_at.dt.weekday\n",
    "        #SORT VALUES\n",
    "        df.sort_values(by=['user_id_day','started_at'], inplace=True, ignore_index=True)\n",
    "        df['cluster_size'] = df['cluster_size'].astype(int)\n",
    "    \n",
    "    if func == func3:\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        for index, row in df.iterrows():\n",
    "            df.loc[index, 'dist_from_home'] = get_distance(row['location_id'], row['home_location_id'], od_matrix_kms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_pickle('sample_data/extended_staypoint_sample_panel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "- `GPSAnalytics().metrics.get_daily_metrics()`\n",
    "\n",
    "Aggregate per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def daily_metrics(df):\n",
    "    \"\"\"\n",
    "    Construct a matrix of daily descriptive statistics.\n",
    "\n",
    "    Args:\n",
    "    - df: DataFrame containing relevant columns from the .\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with computed rhythmic profiles.\n",
    "    \"\"\"\n",
    "    # Select relevant columns\n",
    "    daily_act = df[['user_id_day', 'first_dep', 'last_arr', 'home_loop', 'daily_trip_dist', 'peak', 'num_trip', 'max_dist_from_home', 'weekday']].copy()\n",
    "\n",
    "    # Convert 'first_dep' and 'last_arr' to minutes since midnight\n",
    "    daily_act.loc[daily_act.first_dep.notnull(), 'first_dep'] = (pd.to_datetime(daily_act.loc[daily_act.first_dep.notnull(), 'first_dep'], format=\"%H:%M:%S\") - np.datetime64('1900-01-01')).dt.total_seconds().div(60).astype(int)\n",
    "    daily_act.loc[daily_act.last_arr.notnull(), 'last_arr'] = (pd.to_datetime(daily_act.loc[daily_act.last_arr.notnull(), 'last_arr'], format=\"%H:%M:%S\") - np.datetime64('1900-01-01')).dt.total_seconds().div(60).astype(int)\n",
    "\n",
    "    # Remove duplicate rows based on 'user_id_day'\n",
    "    daily_act.drop_duplicates(subset=['user_id_day'], keep='first', ignore_index=True, inplace=True)\n",
    "\n",
    "    # Create new columns 'am_peak', 'pm_peak', and 'noon_peak'\n",
    "    daily_act['am_peak'] = daily_act['pm_peak'] = daily_act['noon_peak'] = 0\n",
    "    daily_act.loc[daily_act.peak == 'morning_peak', 'am_peak'] = 1\n",
    "    daily_act.loc[daily_act.peak == 'evening_peak', 'pm_peak'] = 1\n",
    "    daily_act.loc[daily_act.peak == 'noon_peak', 'noon_peak'] = 1\n",
    "\n",
    "    # Drop the 'peak' column\n",
    "    daily_act.drop('peak', inplace=True, axis=1)\n",
    "\n",
    "    # Set 'user_id_day' as the index\n",
    "    daily_act.set_index('user_id_day', inplace=True)\n",
    "\n",
    "    return daily_act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_metrics(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
