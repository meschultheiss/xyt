{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c94a42b8-6ab2-4117-bb2f-d773a16d19a8",
   "metadata": {},
   "source": [
    "## Instance: GPStoActionspace()\n",
    "### Input\n",
    "Takes as input the output of `GPSAnalytics().metrics.get_metrics()` -> act (dataframe)\n",
    "\n",
    "### Introduction\n",
    "This notebook applies centrography to the MOBIS data at state 2, focusing on activity-level aggregation.\n",
    "\n",
    "The objective is to generate key metrics characterizing the activity space for a more in-depth exploration of spatial familiarity.\n",
    "\n",
    "Spatial familiarity metrics encompass a composite evaluation of location history, daily activity-space variability, and spatial innovation. Achieving this involves intricate data transformations utilizing advanced point-pattern centrography. Leveraging a dataset with labeled locations, including purpose and visit counts, over a specific time frame, marked point pattern analysis (PPA) facilitates the study of individual action spaces (Baddeley, Rubak, and Turner 2015).\n",
    "\n",
    "The implementation of centrography (utilizing the Python Spatial Analysis library) extracts characteristics to describe the activity space:\n",
    "\n",
    "- **Points**: Marked visited places with counts of visits, purpose labels (home, work, leisure, duties), unique location IDs, and intensity (average number of event points per unit of the convex hull area).\n",
    "- **Centers**: The mean center and weighted mean centers (weighted by the count of visits).\n",
    "- **Distances**: Standard distance, offering a one-dimensional measure of how dispersed visited locations are around their mean center, and the sum of distances from home.\n",
    "- **Shapes**: Standard deviational ellipse, providing a two-dimensional measure of the dispersion of visited locations, and the minimum convex hull of frequently visited places.\n",
    "\n",
    "This approach predominantly relies on the Python library for spatial analysis, PySAL.\n",
    "\n",
    "### Public methods\n",
    "The public methods should be the following:\n",
    "- `GSPtoActionspace().compute_action_space(act, aggreg_method = 'user_id'/'user_id_day',plot_ellipses = False)` -> AS (dataframe) Get from Part 0 and 1 and 2 below\n",
    "- `GSPtoActionspace().covariance_matric(AS)` Get from Part 3 below\n",
    "- `GSPtoActionspace().plot_action_space(act, AS, user_subset = ['CH15029', 'CH16871'], how = 'vignette'/'folium', save = False)` Get from Part 4 below\n",
    "- `GPStoActionspace().inno_rate(mtf_, AS_day, user_id_, phase=None, treatment=None)` Get from Part 5 below\n",
    "\n",
    "### Methodology\n",
    "\n",
    "$$\n",
    "\\text{(Eq. 3) } \\quad\n",
    "Regularity = \\dfrac{n_f + 1}{n} \\text{; with $n_f$ the number of frequently visited locations and $n$ the total number of locations} \n",
    "\\\\ \\text{(Eq. 4) }  \\quad\n",
    "    Frequency=\n",
    "    \\begin{cases}\n",
    "      \\text{'most visited'}, & \\text{for}\\ \\arg\\max({f_i}) \\\\\n",
    "      \\text{'frequent visits'}, & \\text{for}\\ f_i > 0.5 \\cdot \\arg\\max({f_i}) \\\\\n",
    "      \\text{'occasional visits'}, & \\text{for}\\ f_i \\leq 0.5 \\cdot \\arg\\max({f_i}) \\\\\n",
    "      \\text{'visited once'}, & \\text{if}\\ f_i = 1\n",
    "    \\end{cases} \n",
    "    \\text{with $f_i$ the count of visits at location $i$}\n",
    "\\\\ \\text{(Eq. 5) }  \\quad\n",
    "    Proximity =\n",
    "    \\dfrac{SD_{freq}}{SD_{all}} \\text{ is }\n",
    "        \\begin{cases}\n",
    "          > 1 \\text{ for dispersed habitual activity space and close innovative activity space} \\\\\n",
    "          \\approx 1 \\text{ for homogeneous activity spaces} \\\\\n",
    "          < 1 \\text{ for dispersed innovative activity space and close habitual activity space }\n",
    "        \\end{cases} \n",
    "\\\\\n",
    "\\qquad  \\text{Given the coordinates $(x,y)$ of $i$ locations :}\n",
    "\\\\\n",
    "\\qquad  \\text{(5.1) } \\text{with}  \\quad\n",
    "SD_{freq} = \\displaystyle \\sqrt{\\frac{\\sum^n_{i=1}(x_i-x_{home})^2}{n} + \\frac{\\sum^n_{i=1}(y_i-y_{home})^2}{n}}\n",
    "\\text{  } \\forall \\text{nodes } n_i(x_i,y_i)\\in C_f = \\{\\text{'frequently visited places', 'most visited place'}\\}\n",
    "\\\\ \\qquad \\text{(5.2) } \\text{and}  \\quad\n",
    "SD_{all} = \\displaystyle \\sqrt{\\frac{\\sum^n_{i=1}(x_i-x_{home})^2}{n} + \\frac{\\sum^n_{i=1}(y_i-y_{home})^2}{n}}\n",
    "\\text{  } \\forall \\text{nodes } n_i(x_i,y_i) \\in C =\\{\\text{'all visited places}\\}\n",
    "\\\\ \\text{(Eq. 6) }  \\quad\n",
    "    \\textit{Home shift} = \n",
    "    \\sqrt {\\left( {x_{home} - x_{wmc} } \\right)^2 + \\left( {y_{home} - y_{wmc} } \\right)^2 }\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a07ef-3a08-42fd-a517-49624ee2dcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_action_space import *\n",
    "\n",
    "# Standard Libraries\n",
    "import math\n",
    "from datetime import datetime\n",
    "import multiprocessing as mp\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "\n",
    "# Data Manipulation and Analysis\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopandas import sjoin\n",
    "import numpy as np\n",
    "import time\n",
    "import utm\n",
    "import pyproj\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.set_palette(\"Pastel2_r\")\n",
    "cm = sns.light_palette(\"#fff4ac\",as_cmap=True)\n",
    "from matplotlib import pyplot as plt\n",
    "import folium\n",
    "import pyproj\n",
    "\n",
    "# Point Pattern Analysis and Spatial Statistics\n",
    "from pointpats.centrography import hull, mbr, mean_center, weighted_mean_center, manhattan_median, std_distance, euclidean_median, ellipse, dtot\n",
    "from shapely.geometry import MultiPoint\n",
    "\n",
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "from haversine import haversine, Unit\n",
    "from matplotlib.patches import Ellipse\n",
    "from pylab import figure, show, rand\n",
    "\n",
    "from pointpats import PointPattern\n",
    "from scipy.spatial import distance\n",
    "from pysal.explore.pointpats import mean_center, weighted_mean_center, std_distance, euclidean_median, ellipse\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "import libpysal as ps\n",
    "from pointpats import PointPattern\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.rcParams['figure.dpi'] = 120 # 200 e.g. is really fine, but slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e85163-7403-4269-bb16-961714b911cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "act = pd.read_pickle('sample_data/extended_staypoint_sample_panel.pkl')\n",
    "act.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91679489-e6c9-4dc6-a0df-5a9d67f586ba",
   "metadata": {},
   "source": [
    "### Part 0\n",
    "Preprocess the input so rest of the finctions work\n",
    "\n",
    "maybe better to add this line directly in the GPSAnalytics().get_metrics method ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172c088-4026-4fee-b251-0472a317c44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "act.cluster_size = act.cluster_size.astype(int) #make sure we have the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37e2d7-72b9-4a8f-b728-f8e959f65727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I need to get the MAIN home location ID first, as in the data above one user_id may have different home location for each day.\n",
    "\n",
    "# Group by 'user_id' and the date part of 'started_at'\n",
    "grouped = act.groupby(['user_id', act['started_at'].dt.date])\n",
    "\n",
    "# Find the most recurrent 'home_location_id' for each user and day\n",
    "most_recurrent_home = grouped['home_location_id'].agg(lambda x: x.value_counts().idxmax())\n",
    "\n",
    "# Most recurrent home_id per user\n",
    "most_recurrent_home_id = most_recurrent_home.value_counts().idxmax()\n",
    "\n",
    "# Create a mapping of user_id and date to the most recurrent home_location_id\n",
    "mapping = most_recurrent_home.reset_index().set_index(['user_id','started_at']).to_dict()['home_location_id']\n",
    "\n",
    "# Map the values to the original DataFrame to create the new column\n",
    "act['main_home_location_id'] = act.set_index(['user_id', act['started_at'].dt.date]).index.map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf16fc0-74f4-44e3-8046-c856e22f8e73",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "Process the action space metrics per user_id (all days, one_user) or per user_id_day (one day, one user)\n",
    "- `GSPtoActionspace().compute_action_space(act, aggreg_method = 'user_id'/'user_id_day',plot_ellipses = False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec13b8f-a034-4b9a-8a05-6caa2a0fdca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_level = 'user_id_day'  # Change this to 'user_id' or 'user_id_day'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c613f40-defe-40e5-8b0c-c12a8003edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = []\n",
    "cols = ['user_id', 'lon', 'lat', 'cluster_size', 'cluster_info', 'location_id', 'home_location_id', 'num_trip', 'main_home_location_id', 'dist_from_home', 'imputed_purpose']\n",
    "\n",
    "\n",
    "for user_id_ in act[aggregation_level].unique():\n",
    "    \n",
    "    act_ = act.loc[act[aggregation_level] == user_id_, cols].copy()\n",
    "    # get the main home id at user_id scale\n",
    "    main_home_id = act_.home_location_id.value_counts().idxmax()\n",
    "    \n",
    "    if main_home_id == 'not_detected':\n",
    "        continue\n",
    "    #elif np.any(act_.dist_from_home > 3000000):\n",
    "    #    continue\n",
    "    elif act_.main_home_location_id.unique() != act_.home_location_id.unique():\n",
    "        continue\n",
    "    elif np.all(act_.num_trip == 0):\n",
    "        continue\n",
    "    else:\n",
    "        act_.drop(columns=['num_trip', 'dist_from_home'], inplace=True)\n",
    "        \n",
    "        # Assuming your DataFrame is called df\n",
    "        act_ = gpd.GeoDataFrame(act_, geometry=gpd.points_from_xy(act_['lon'], act_['lat']), crs=\"EPSG:4326\")\n",
    "        \n",
    "        # Change the CRS to EPSG:2056\n",
    "        act_ = act_.to_crs(\"EPSG:2056\")  # This should be a parameter to change in the method, but default set to this one\n",
    "        \n",
    "        # Now, you can access the transformed x and y coordinates\n",
    "        act_['x'], act_['y'] = act_.geometry.x, act_.geometry.y\n",
    "        \n",
    "        # And drop duplicated locations (information of visit count is in cluster_size)\n",
    "        act_ = act_.drop_duplicates(ignore_index=True)\n",
    "        \n",
    "        # Differentiate most frequent locations from all locations\n",
    "        act_freq = act_.loc[act_.cluster_info.isin(['Most visited', 'Frequent visit'])].reset_index(drop=True)\n",
    "        \n",
    "        if len(act_) == 0:\n",
    "            continue\n",
    "        else:\n",
    "        \n",
    "            # Find mean center of a point array\n",
    "            mc_ = mean_center(act_[['x', 'y']])\n",
    "            # Find weighted mean center of a marked point pattern.\n",
    "            wmc_ = weighted_mean_center(act_[['x', 'y']], act_.cluster_size)\n",
    "            \n",
    "            # Calculate Args of standard deviational ellipse for a point pattern\n",
    "            if len(act_) > 2:\n",
    "                sx, sy, theta = pointpats.centrography.ellipse(act_[['x', 'y']])\n",
    "                theta_degree = np.degrees(theta)  # need degree of rotation to plot the ellipse\n",
    "                # Calculate the surface of the SD ellipse of the action space, in square meters\n",
    "                surface_ellipse = np.pi * sx * sy\n",
    "            else:\n",
    "                surface_ellipse = 0\n",
    "            # sx, sy, theta, theta_degree\n",
    "            \n",
    "            # Calculate the ellipse motplotlib objet\n",
    "            e = Ellipse(xy=wmc_, width=sx*2, height=sy*2, angle=-theta_degree, alpha=0.2, facecolor='#c9cfbc') #angle is rotation in degrees (anti-clockwise)\n",
    "            \n",
    "            # Find the main home coordinates\n",
    "            find_home_geom = act_.loc[act_.location_id == act_.home_location_id.unique()[0], 'geometry']\n",
    "            if len(find_home_geom) == 1:\n",
    "                home_loc = np.array([find_home_geom.x, find_home_geom.y], dtype=float)\n",
    "            elif len(find_home_geom) > 1:\n",
    "                home_loc = np.array([find_home_geom[0].x, find_home_geom[0].y], dtype=float)\n",
    "            \n",
    "            # Sum of Euclidean distances between event points and a selected point.\n",
    "            sum_dist_home = dtot(home_loc, act_[['x', 'y']].to_numpy())\n",
    "            \n",
    "            # Compute location regularity\n",
    "            n_all = len(act_)\n",
    "            n_freq = len(act_freq)\n",
    "            regularity = n_freq / n_all\n",
    "            \n",
    "            # Calculate standard distance between all nodes and home\n",
    "            std_dist_all = modified_std_distance(act_, home_loc)\n",
    "            # Calculate standard distance between freq nodes and home\n",
    "            if len(act_freq) > 0:\n",
    "                std_dist_freq = modified_std_distance(act_freq, home_loc)\n",
    "            else:\n",
    "                std_dist_freq = np.nan\n",
    "            \n",
    "            # Calculate the proximity\n",
    "            proximity = std_dist_freq / std_dist_all\n",
    "            \n",
    "            # Calculate the Euclidean median for a point pattern.\n",
    "            median_ = euclidean_median(act_[['x', 'y']].to_numpy())[0]\n",
    "            \n",
    "            # Get area of convex hull for frequent (i.e. habitual) locations in square meters\n",
    "            # And Coefficient of intensity\n",
    "            if len(act_freq.location_id.unique()) > 2:\n",
    "                freq_hull_area = MultiPoint(act_freq.geometry).convex_hull.area\n",
    "                intensity = MultiPoint(act_freq.geometry).convex_hull.area / MultiPoint(act_.geometry).convex_hull.area\n",
    "            elif len(act_freq.location_id.unique()) == 2:\n",
    "                loc1 = act_freq[['x', 'y']].loc[act_freq.location_id == act_freq.location_id.unique()[0]].drop_duplicates()\n",
    "                loc2 = act_freq[['x', 'y']].loc[act_freq.location_id == act_freq.location_id.unique()[1]].drop_duplicates()\n",
    "            \n",
    "                coord1 = loc1[['x', 'y']].to_numpy().flatten()\n",
    "                coord2 = loc2[['x', 'y']].to_numpy().flatten()\n",
    "            \n",
    "                freq_hull_area = distance.euclidean(coord1, coord2)\n",
    "                intensity = 0\n",
    "            else:\n",
    "                freq_hull_area = 0\n",
    "                intensity = 0\n",
    "            \n",
    "            # Calculate euclidean distance between weighted mean center and main home location in meters\n",
    "            home_shift = distance.euclidean(wmc_, home_loc.flatten())\n",
    "            \n",
    "\n",
    "            action_space_ = np.array(\n",
    "                [user_id_, main_home_id, sum_dist_home, proximity, std_dist_all, std_dist_freq, median_, intensity,\n",
    "                 regularity, n_all, n_freq, freq_hull_area, home_shift, e, surface_ellipse,wmc_[0],wmc_[1],sx,sy,theta_degree])\n",
    "            action_space.append(action_space_)\n",
    "\n",
    "columns_ = [aggregation_level, 'main_home_id', 'sum_dist_home', 'proximity', 'std_dist_all', 'std_dist_freq', 'median_', 'intensity', 'regularity', 'n_all', 'n_freq', 'freq_hull_area', 'home_shift', 'ellipse_2056', 'surface_ellipse','wmc_x','wmc_y','sx','sy','theta_degree']\n",
    "AS = pd.DataFrame(action_space, columns=columns_)\n",
    "AS.set_index(aggregation_level,inplace=True)\n",
    "\n",
    "#Parse to float\n",
    "col_2= ['sum_dist_home','proximity','std_dist_all', 'std_dist_freq','median_','intensity','regularity','n_all','n_freq','freq_hull_area','home_shift','surface_ellipse']\n",
    "AS[col_2] = AS[col_2].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2156571-0bc0-4a11-bdf6-8ba0e932a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "AS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4968d4b9-5ebc-42bf-8375-0c745ab57f04",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "Plot the ellipses, get one color per user and user_id on hover.\n",
    "\n",
    "`MATTTEO` Please integrate this the previous methods in Past 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce55201-da81-428c-9fe8-cd5d9a2ced52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, Point\n",
    "import folium\n",
    "import pyproj\n",
    "\n",
    "# Create a list to store ellipse geometries\n",
    "ellipse_geometries = []\n",
    "\n",
    "# Define the source and target coordinate reference systems (CRS)\n",
    "source_crs = \"EPSG:2056\"\n",
    "target_crs = \"EPSG:4326\"\n",
    "\n",
    "# Iterate through the DataFrame and add ellipses to the list\n",
    "for idx, row in AS.reset_index().iterrows():\n",
    "    # Extract ellipse information\n",
    "    user_id = row[aggregation_level]\n",
    "    ellipse = row['ellipse_2056']\n",
    "\n",
    "    # Convert ellipse to Polygon geometry\n",
    "    ellipse_polygon = Polygon(ellipse.get_verts())\n",
    "    ellipse_coords = [[x,y] for x,y in ellipse_polygon.exterior.coords]\n",
    "\n",
    "    ellipse_polygon = Polygon(ellipse.get_verts())\n",
    "    ellipse_geometries.append(ellipse_polygon)\n",
    "\n",
    "# Create a GeoDataFrame from the list of ellipse geometries\n",
    "ellipse_gdf = gpd.GeoDataFrame(geometry=ellipse_geometries, crs=source_crs).to_crs(target_crs)\n",
    "\n",
    "# Calculate the mean centroid of all ellipses\n",
    "mean_lat = ellipse_gdf.geometry.apply(lambda geom: geom.centroid.x).mean()\n",
    "mean_lon = ellipse_gdf.geometry.apply(lambda geom: geom.centroid.y).mean()\n",
    "\n",
    "# Create a Folium map centered around the mean centroid\n",
    "mymap = folium.Map(location=[mean_lon, mean_lat], zoom_start=12, control_scale=True)\n",
    "\n",
    "# Iterate through the GeoDataFrame and add ellipses to the map\n",
    "for idx, row in ellipse_gdf.iterrows():\n",
    "    # Extract ellipse information\n",
    "    user_id = idx  # Assuming user_id is the index in the GeoDataFrame\n",
    "    ellipse_polygon = row['geometry']\n",
    "\n",
    "    # Convert ellipse to coordinates for Folium\n",
    "    ellipse_coords = list(ellipse_polygon.exterior.coords)\n",
    "    ellipse_coords_t = [[y,x] for x,y in ellipse_coords]\n",
    "\n",
    "    # Add the ellipse to the map\n",
    "    folium.Polygon(locations=ellipse_coords_t, color='blue', fill=True, fill_color='blue', fill_opacity=0.02, popup=f\"User ID: {user_id}\").add_to(mymap)\n",
    "\n",
    "# Display the map\n",
    "mymap#.save(\"ellipse_map.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef1c02f-684d-4495-a6d3-f83857ae0ea2",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "Covariance matrix of the Action Space Args we computed\n",
    "- `GSPtoActionspace().covariance_matric(AS)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d880edd-0c7a-4a00-95ef-7d7e475770f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_corr_heatmap(AS[['sum_dist_home','proximity','std_dist_all', 'std_dist_freq','median_','intensity','regularity','n_all','n_freq','freq_hull_area','home_shift','surface_ellipse']], \n",
    "                 method='corr',cmap = sns.diverging_palette(360,65, l=80, as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9339c28b-6bf0-4b28-bf1f-20ba191f23e7",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "More plots, including the points from `act` and ellipse from `AS`\n",
    "- `GSPtoActionspace().plot_action_space(act, AS, user_subset = ['CH15029', 'CH16871'], how = 'vignette'/'folium', save = False)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c61561b-e37a-4857-9c0b-42916f5e848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "\n",
    "# Create a GeoDataFrame for the points from act_\n",
    "print(act_.head())\n",
    "geometry = [Point(x, y) for x, y in zip(act_['x'], act_['y'])]\n",
    "gdf = gpd.GeoDataFrame(act_, geometry=geometry, crs='EPSG:2056')\n",
    "\n",
    "# Create a Matplotlib figure\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "\n",
    "# Define the source and target coordinate reference systems (CRS)\n",
    "#source_crs = \"EPSG:2056\"\n",
    "#target_crs = \"EPSG:4326\"\n",
    "## Create a PyProj transformer for the conversion\n",
    "#transformer = pyproj.Transformer.from_crs(source_crs, target_crs, always_xy=True)\n",
    "\n",
    "# Plot Ellipse\n",
    "e = Ellipse(xy=wmc_, width=sx*2, height=sy*2, angle=-theta_degree, alpha=0.2, facecolor='#c9cfbc') #angle is rotation in degrees (anti-clockwise)\n",
    "ellipse_polygon = Polygon(e.get_verts())\n",
    "ellipse_coords = np.array(ellipse_polygon.exterior.coords.xy)\n",
    "#ellipse_coords_4326 = np.column_stack(transformer.transform(x, y) for x, y in zip(ellipse_coords[0], ellipse_coords[1]))\n",
    "ax.fill(ellipse_coords[0], ellipse_coords[1], color='red', alpha=0.2, edgecolor='red')\n",
    "\n",
    "# Plot points from act_\n",
    "ax.scatter(gdf['x'], gdf['y'], color='red', marker='o', s=50, label='Points from act_')\n",
    "\n",
    "# Plot wmc_ in green\n",
    "ax.scatter(wmc_[0], wmc_[1], color='green', marker='o', s=50, label='Weighted mean center')\n",
    "\n",
    "# Plot home_loc from act_ with a customized icon\n",
    "ax.scatter(home_loc[0], home_loc[1], color='black', marker='^', s=100, label='Home Location')\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "\n",
    "# Set the extent of the plot based on the bounding box of the GeoDataFrame\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "ax.set_xlim(minx * 0.998, maxx * 1.002)\n",
    "ax.set_ylim(miny * 0.998, maxy * 1.002)\n",
    "\n",
    "# Plot basemap using OpenStreetMap\n",
    "ctx.add_basemap(ax, crs='EPSG:2056', source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f17375-b989-471b-ab65-d7b6973877cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster, Draw\n",
    "import numpy as np\n",
    "\n",
    "# Convert wmc_ to 'EPSG:4326'\n",
    "wmc_4326 = gpd.GeoSeries(Point(wmc_[0], wmc_[1]), crs='EPSG:2056').to_crs(\"EPSG:4326\").iloc[0]\n",
    "wmc_4326 = [wmc_4326.y, wmc_4326.x]  # Flip lat and lon to match Folium format\n",
    "\n",
    "# Create a GeoDataFrame for the points from act_\n",
    "geometry = [Point(lon, lat) for lon, lat in zip(act_['lon'], act_['lat'])]\n",
    "gdf = gpd.GeoDataFrame(act_, geometry=geometry, crs='EPSG:4326')\n",
    "\n",
    "# Create a GeoDataFrame for the home_loc\n",
    "home_loc_geometry = [Point(home_loc[0], home_loc[1])]\n",
    "home_loc_gdf = gpd.GeoDataFrame(geometry=home_loc_geometry, crs='EPSG:2056').to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Create a GeoDataFrame for the ellipse\n",
    "# Define the source and target coordinate reference systems (CRS)\n",
    "source_crs = \"EPSG:2056\"\n",
    "target_crs = \"EPSG:4326\"\n",
    "# Create a PyProj transformer for the conversion\n",
    "transformer = pyproj.Transformer.from_crs(source_crs, target_crs, always_xy=True)\n",
    "\n",
    "# Create Ellipse\n",
    "e = Ellipse(xy=wmc_, width=sx*2, height=sy*2, angle=-theta_degree, alpha=0.2, facecolor='#c9cfbc') #angle is rotation in degrees (anti-clockwise)\n",
    "ellipse_polygon = Polygon(e.get_verts())\n",
    "ellipse_coords = [[x,y] for x,y in ellipse_polygon.exterior.coords]\n",
    "# Convert coordinates to EPSG:4326\n",
    "ellipse_coords_4326 = [transformer.transform(x, y) for x, y in ellipse_coords]\n",
    "ellipse_coords_4326_t = [[y,x] for x,y in ellipse_coords_4326]\n",
    "ellipse_gdf = gpd.GeoDataFrame(geometry=[Polygon(ellipse_coords_4326)], crs='EPSG:4326')\n",
    "\n",
    "# Create a Folium map centered around the mean coordinates\n",
    "mean_lat, mean_lon = gdf['lat'].mean(), gdf['lon'].mean()\n",
    "map_center = [mean_lat, mean_lon]\n",
    "mymap = folium.Map(location=map_center, zoom_start=12, control_scale=True)\n",
    "\n",
    "# Add basemap using OpenStreetMap\n",
    "folium.TileLayer('openstreetmap').add_to(mymap)\n",
    "\n",
    "# Add Ellipse\n",
    "#folium.Polygon(locations=ellipse.exterior.coords, color='#c9cfbc', fill=True, fill_color='#c9cfbc', fill_opacity=0.2).add_to(mymap)\n",
    "folium.GeoJson(ellipse_gdf, name='polygon',zoom_on_click=True, style_function=lambda x: {'color': 'red'}).add_to(mymap)\n",
    "\n",
    "# Add points from act_ with CircleMarkers\n",
    "for idx, row in gdf.iterrows():\n",
    "    folium.CircleMarker(location=[row['lat'], row['lon']], radius=5, color='red', fill=True, fill_color='red', fill_opacity=0.7, popup=f\"{row['user_id']}\").add_to(mymap)\n",
    "\n",
    "# Add wmc_ in green\n",
    "folium.CircleMarker(location=wmc_4326, radius=5, color='green', fill=True, fill_color='green', fill_opacity=1, popup=\"Weighted mean ceter\").add_to(mymap)\n",
    "\n",
    "# Add home_loc from act_ with a customized icon\n",
    "folium.Marker(location=[home_loc_gdf.geometry.y.iloc[0], home_loc_gdf.geometry.x.iloc[0]], icon=folium.Icon(color='red', icon='home')).add_to(mymap)\n",
    "\n",
    "# Display the map\n",
    "mymap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be2366-28d5-4987-bad9-d5a01cdf9b66",
   "metadata": {},
   "source": [
    "### Part 5\n",
    "**Method**\n",
    "- `GPStoActionspace().inno_rate(mtf_, AS_day, user_id_, phase=None, treatment=None)`\n",
    "\n",
    "**Objective**\n",
    "\n",
    "The objective here is to plot the innovation rate\n",
    "\n",
    "**The necessary data for this method are :**\n",
    "- `GPStoGraph().get_graphs()` -> mtf_ \n",
    "- `GSPtoActionspace().compute_action_space(act, aggreg_method = 'user_id_day')` -> AS_day\n",
    "\n",
    "**phase and treatment**\n",
    "\n",
    "GPS data often come with a treatment e.g. {control, treat_1, treat_2} or phase column e.g. {before, after}. Here it is not the case, so no hue is necessary on the plots. But please keep the option of plotting hue for different treatment or phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f84696-5a6c-4ad3-9ecc-b32663a0a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtf_treatment = pd.merge(mtf_[['DiGraph_motif','motif_flat']], AS_day.set_index('user_id_day'), left_index=True, right_index=True, how='left')\n",
    "mtf_treatment.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4e6a10-487f-4afe-b506-14373521aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inno_rate_per_phase(mtf_useridday, user_id_, col_digraph_motif = 'DiGraph_motif', phase=None, treatment=None):\n",
    "    '''\n",
    "    Compute the innovation rate from motif\n",
    "    user_id: list of users to compute the innovation rate\n",
    "    mtf_useridday: df of DiGraph_motifs with user_id_days in index'''\n",
    "\n",
    "    #df_innov_rate = pd.DataFrame(index=range(100),columns=user_id)\n",
    "\n",
    "    #isolate rows for phase X only\n",
    "    if phase != None:\n",
    "        mtf_useridday = mtf_useridday[mtf_useridday.phase == phase]\n",
    "    #isolate rows for treatment X only\n",
    "    if treatment != None:\n",
    "        mtf_useridday = mtf_useridday[mtf_useridday.treatment == treatment]\n",
    "    #resample for continuous days\n",
    "    graphs = mtf_useridday[mtf_useridday.user_id == user_id_].set_index('date').resample('D').ffill()\n",
    "    #read the first graph G0 for user_id\n",
    "    G0 = graphs[col_digraph_motif][0]\n",
    "    #init inno rate list\n",
    "    innovation_rate = []\n",
    "    #then compute the next graph and compare with G0\n",
    "    for G in graphs[col_digraph_motif][1:]:\n",
    "        innovation_rate.append(nx.compose(G0,G).number_of_nodes())\n",
    "        G0 = nx.compose(G0,G)\n",
    "    #x = np.arange(0, len(innovation_rate), 1).tolist()\n",
    "    #y = innovation_rate\n",
    "    \n",
    "    return innovation_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78777eb8-3e1b-49b3-8516-e60fa0100e95",
   "metadata": {},
   "source": [
    "the script below is implemented in case of phase and treatment. In the sample data I have no phase and no treatment, but I still want to be able to plot the innovation rate. Sorry it is messy. It you cannot adapt I will do it myself later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff646114-2943-47d6-8c7d-2e8fca527af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = 800\n",
    "#Init df to be populated with the innovation rates per cluster\n",
    "df_innov_rate_1 = pd.DataFrame(index=range(500),columns=user_id_clstr1[:to_plot]) \n",
    "df_innov_rate_2 = pd.DataFrame(index=range(500),columns=user_id_clstr2[:to_plot]) \n",
    "df_innov_rate_3 = pd.DataFrame(index=range(500),columns=user_id_clstr3[:to_plot]) \n",
    "\n",
    "\n",
    "treatment_ = 'Pricing' #Pricing, Nudging, Control\n",
    "\n",
    "\n",
    "#Init df to have the mean innovation rates in a single df\n",
    "mean_innov_rate = pd.DataFrame(index=range(500),columns=['exclusive_phase1', 'moderate_phase1', 'mixed_phase1','exclusive_phase2', 'moderate_phase2', 'mixed_phase2'])\n",
    "\n",
    "for phase_ in [1,2]:\n",
    "    #Exclusive car users, user_id_clstr1\n",
    "    for user_id_ in user_id_clstr1[:to_plot]:\n",
    "        try:\n",
    "            y = get_inno_rate_per_phase(mtf_treatment, user_id_, phase=phase_, treatment=treatment_)\n",
    "            x = np.arange(0, len(y), 1).tolist()\n",
    "            df_innov_rate_1.loc[x, user_id_] = y\n",
    "        except:\n",
    "            continue        \n",
    "    \n",
    "    mean_innov_rate.loc[:, 'exclusive_phase%d' %phase_] = df_innov_rate_1.mean(axis=1)\n",
    "    \n",
    "    #Moderate car users, user_id_clstr2\n",
    "    for user_id_ in user_id_clstr2[:to_plot]:\n",
    "        try:\n",
    "            y = get_inno_rate_per_phase(mtf_treatment, user_id_, phase=phase_, treatment=treatment_)\n",
    "            x = np.arange(0, len(y), 1).tolist()\n",
    "            df_innov_rate_2.loc[x, user_id_] = y\n",
    "        except:\n",
    "            continue        \n",
    "    \n",
    "    mean_innov_rate.loc[:, 'moderate_phase%d' %phase_] = df_innov_rate_2.mean(axis=1)\n",
    "\n",
    "    #Mixed car users, user_id_clstr3\n",
    "    for user_id_ in user_id_clstr3[:to_plot]:\n",
    "        try:\n",
    "            y = get_inno_rate_per_phase(mtf_treatment, user_id_, phase=phase_, treatment=treatment_)\n",
    "            x = np.arange(0, len(y), 1).tolist()\n",
    "            df_innov_rate_3.loc[x, user_id_] = y\n",
    "        except:\n",
    "            continue        \n",
    "    \n",
    "    mean_innov_rate.loc[:, 'mixed_phase%d' %phase_] = df_innov_rate_3.mean(axis=1)\n",
    "\n",
    "mean_innov_rate.head(3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f593f-80b1-4fae-9410-3db7e8082ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_innov_rate_cluster1 = sns.lineplot(data=mean_innov_rate[['exclusive_rate_phase1','moderate_rate_phase1', 'mixed_rate_phase1']][:25]) #, legend=False\n",
    "#plot_innov_rate_cluster1.get_figure().savefig(\"innov_rate_modal_clus__phase1%s.png\"%treatment_, dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
